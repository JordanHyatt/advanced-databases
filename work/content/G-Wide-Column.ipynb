{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c97497b-ae95-41d7-8eb8-d0cc6f6bd282",
   "metadata": {},
   "source": [
    "# Unit G\n",
    "# Wide-Column Database Model\n",
    "\n",
    "- Examples From Video Lecture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c984c4b4-a45e-45f6-9d77-d80b2f32b0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "# CASSANDRA CONFIGURATION\n",
    "cassandra_host = \"cassandra\"\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName('jupyter-pyspark') \\\n",
    "      .config(\"spark.cassandra.connection.host\", cassandra_host) \\\n",
    "      .config(\"spark.jars.packages\",\"com.datastax.spark:spark-cassandra-connector-assembly_2.12:3.1.0\")\\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff31bf36-6522-45ff-972a-673fb27289dc",
   "metadata": {},
   "source": [
    "## CASSANDRA CQL\n",
    "\n",
    "- SQL-Like Syntax for Cassandra\n",
    "\n",
    "\n",
    "### Connecting to the Cassandra client\n",
    "\n",
    "`PS> docker-compose exec cassandra cqlsh`\n",
    "\n",
    "\n",
    "### Keyspaces\n",
    "\n",
    "```\n",
    "Cqsql> describe keyspaces;\n",
    "Cqlsh> create keyspace sysmon with replication = { 'class' : 'SimpleStrategy', 'replication_factor' : 1 };\n",
    "Cqlsh> Use sysmon;\n",
    "Cqlsh> HELP\n",
    "\n",
    "```\n",
    "\n",
    "### Table Basics\n",
    "\n",
    "All from the `cqlsh:sysmon>` prompt\n",
    "\n",
    "```\n",
    "# let’s make a table - every taable MUST have a primary key!! name is BOTH partion and cluster key\n",
    "\n",
    "CREATE table users (name text, age tinyint, primary key (name));\n",
    "\n",
    "# take a look at it \n",
    "DESCRIBE table users;\n",
    "\n",
    "# you can drop tables \n",
    "Drop table users;\n",
    "\n",
    "# now re-create the table\n",
    "\n",
    "# Some inserts – you can insert the same thing\n",
    "Insert into users (name, age) values (‘mike’, 47);\n",
    "Insert into users (name, age) values (‘mike’, 47);\n",
    "Insert into users (name, age) values (‘mike’, 47);\n",
    "\n",
    "# show data\n",
    "Select * from users;\n",
    "\n",
    "#what happens when you insert the same key with different values? – its like an update!\n",
    "Insert into users (name, age) values (‘mike’, 48);\n",
    "\n",
    "\n",
    "# but its only there once – There are no integrity constraints!\n",
    "# every INSERT is an UPSERT. INSERT or UPDATE IF EXISTS\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "### Understanding Partitioning\n",
    "\n",
    "Let's look at a more realistic example:\n",
    "\n",
    "```\n",
    "CREATE TABLE system_utilization (\n",
    "\thostname TEXT,\n",
    "\tos TEXT,\n",
    "\tmeasured_on TIMESTAMP,\n",
    "\tcpu_pct TINYINT,\n",
    "\tPRIMARY KEY (hostname, measured_on)\n",
    ");\n",
    "\n",
    "#\n",
    "Insert into system_utilization (hostname, os, measured_on, cpu_pct) values ('Saturn', 'windows', '2018-07-19 09:00', 90);\n",
    "Insert into system_utilization (hostname, os, measured_on, cpu_pct) values ( 'Saturn', 'windows', '2018-07-19 10:00', 5);\n",
    "Insert into system_utilization (hostname, os, measured_on, cpu_pct) values ( 'Saturn', 'windows', '2018-07-19 11:00', 10);\n",
    "Insert into system_utilization (hostname, os, measured_on, cpu_pct) values ( 'venus', 'osx', '2018-07-19 09:00', 5);\n",
    "Insert into system_utilization (hostname, os, measured_on, cpu_pct) values ( 'venus', 'osx', '2018-07-19 10:00', 0);\n",
    "Insert into system_utilization (hostname, os, measured_on, cpu_pct) values ( 'venus', 'osx', '2018-07-19 11:00', 15);\n",
    "Insert into system_utilization (hostname, os, measured_on, cpu_pct) values ( 'mars', 'windows', '2018-07-19 09:00', 5);\n",
    "Insert into system_utilization (hostname, os, measured_on, cpu_pct) values ( 'mars', 'windows', '2018-07-19 10:00', 50);\n",
    "Insert into system_utilization (hostname, os, measured_on, cpu_pct) values ( 'mars', 'windows', '2018-07-19 11:00', 75);\n",
    "\n",
    "# you can filter by partition key\n",
    "Select * from system_utilization where hostname ='mars';\n",
    "\n",
    "# you can filter by particion key and cluster key\n",
    "Select * from system_utilization where hostname ='mars' and measured_on > '2018-07-19 9:30';\n",
    "\n",
    "#you cannot filter by non key\n",
    "Select * from system_utilization where os = 'windows'\n",
    "\n",
    "# or even just by cluster key\n",
    "Select * from system_utilization where measured_on >'2018-07-19 9:30';\n",
    "\n",
    "# or a combination of partition key plus a non-key\n",
    "Select * from system_utilization where hostname ='mars' and cpu_pct=5;\n",
    "\n",
    "# youc can add  aLLOW FILTERING to override this but need to consider what you are doing! \n",
    "\n",
    "# This is why the partitioning scheme is so important to Cassandra queries This data was partitioned so that each hostname is on its own node in the cluster. So when we omit hostname from the query Cassandra must ask every node for its data. This can be quite time consuming when there are dozens of nodes!\n",
    "```\n",
    "\n",
    "### Secondary Indexes\n",
    "\n",
    "Indexes permit querrying non-key columns when the parition key is specified.\n",
    "\n",
    "```\n",
    "# Needs allow fildtering\n",
    "Select * from system_utilization where os='windows' and hostname='Saturn';\n",
    "\n",
    "# Create the index \n",
    "Create index ix_system_utilization_os ON system_utilization (os);\n",
    "\n",
    "# now these queries work without the ALLOW FILTERING\n",
    "Select * from system_utilization where os='windows' and hostname='Saturn';\n",
    "\n",
    "# You can leave off the hostname and it works, but this is a bad idea since indexes are distributed.\n",
    "Select * from system_utilization where os=‘windows’;\n",
    "\n",
    "# to show the index on the table; It’s attached to it!\n",
    "Describe system_utilitzation\n",
    "\n",
    "# drop the index\n",
    "Drop index ix_system_utilization_os\n",
    "```\n",
    "\n",
    "### Materialized Views\n",
    "\n",
    "Mat Views re-write the partition key so the same data may be queried in different ways.\n",
    "\n",
    "\n",
    "```\n",
    "# Create materialized view\n",
    "Create materialized view system_utilization_by_os \n",
    "\tas \n",
    "\tselect * from system_utilization where os is not null and hostname is not null and measured on is not null primary key (os, hostname, measured_on);\n",
    "\u000b",
    "#let’s see the view attached to the table\n",
    "Desc system_utilization;\n",
    "\n",
    "# Can’t filter on the os column without ALLOW FILTERING\n",
    "Select * from system_utilization where os = 'osx';\n",
    "\n",
    "#but you can filter by os on the MV since it has a new partition key!\n",
    "Select * from system_utilization_by_os where os = 'osx';\n",
    "\n",
    "#insert to show it works!\n",
    "Insert into system_utilization (hostname, os, measured_on, cpu_pct) values ( 'earth', 'osx', '2018-07-19 9:00', 100);\n",
    "\n",
    "# Select from table / and materliized view as proof\n",
    "Select * from system_utilization_by_os where os = 'osx';\n",
    "\n",
    "# This can be costly as we are duplicating a lot of data. at least the API allows us to insert once as opposed to separate tables!\n",
    "```\n",
    "\n",
    "\n",
    "### Updates and Deletes \n",
    "\n",
    "```\n",
    "#alter table\n",
    "Alter table systems_utilization add applications list<text>;\n",
    "\n",
    "Update systems_utilizations set applications = ['word', 'excel'] where hostname ='mars' and measured_on '2019-07-19 9:00'\n",
    "\n",
    "Update systems_utilizations set applications = ['calc', 'word'] where hostname ='mars' and measured_on '2019-07-19 10:00'\n",
    "\n",
    "Update systems_utilizations set applications = ['calc', 'solitare', 'doom'] where hostname ='mars' and measured_on ‘2019-07-19 11:00'\n",
    "\n",
    "Select * from systems_utilization where applications contains 'word' allow filtering;\n",
    "\n",
    "#set it to null\n",
    "Delete applications from systems_utilization where hostname ='mars' and measured_on '2019-07-19 11:00'\n",
    "\n",
    "Select * from systems_utilizations;\n",
    "\n",
    "# delete row\n",
    "Delete from systems_utilizations where hostname ='mars' and measured_on '2019-07-19 11:00'\n",
    "```\n",
    "\n",
    "### Consistency Levels\n",
    "\n",
    "```\n",
    "Consistency\n",
    "Consistency all\n",
    "Consistency quorum\n",
    "Consistency one;\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c0108a-48b9-474b-869b-9a1eea2f478e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ad9de63f-c8a4-417a-9de6-485d1417fd09",
   "metadata": {},
   "source": [
    "## Loading Sample Data\n",
    "\n",
    "- Run this code to load some sample data into Cassandra\n",
    "- Cassandra requires a schema. You cannot create this schema in Spark, so we will use plain-old-python to run the CQL DDL code to make the table\n",
    "- The CSV file does not understand dates, so we must use `withColumn()` to cast the string type to date type prior to loading in cassandra.\n",
    "- Because Cassandra is key based, and supports UPSERT we can use the \"Append\" mode to write data to the table.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f59cfb5a-5564-4aae-84ef-631cb48c27a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WE NEED A TABLE BEFORE WE CAN WRITE, Using Plain old Python\n",
    "!pip install -q cassandra-driver\n",
    "from cassandra.cluster import Cluster\n",
    "with Cluster([cassandra_host]) as cluster:\n",
    "    session = cluster.connect()\n",
    "    session.execute(\"CREATE KEYSPACE IF NOT EXISTS gdemo WITH replication={ 'class': 'SimpleStrategy', 'replication_factor' : 1 };\")\n",
    "    table = '''\n",
    "    CREATE TABLE IF NOT EXISTS gdemo.fudgemart_order_details (\n",
    "        customer_id int,\n",
    "        customer_email text,\n",
    "        customer_name text,\n",
    "        customer_address text,\n",
    "        customer_city text,\n",
    "        customer_state text,\n",
    "        customer_zip text,\n",
    "        order_id int,\n",
    "        order_date date,\n",
    "        creditcard_number text,\n",
    "        creditcard_exp_date text, \n",
    "        order_total decimal ,\n",
    "        ship_via text,\n",
    "        shipped_date date,\n",
    "        product_id int,\n",
    "        order_item_id int,\n",
    "        order_qty int,\n",
    "        product_name text,\n",
    "        product_retail_price decimal,\n",
    "    primary key ((customer_id, order_id), order_item_id) \n",
    "    );\n",
    "    '''\n",
    "    session.execute(table)\n",
    "\n",
    "# NOTE: CSV File format does not understand dates, but Cassandra does, so we must cast the string columns to date before loading into the table\n",
    "od = spark.read.option(\"inferSchema\",True).option(\"header\",True).csv(\"file:///home/jovyan/datasets/fudgemart/fudgemart-order-details.csv\")\\\n",
    "    .withColumn(\"order_date\", col(\"order_date\").cast(\"date\")).withColumn(\"shipped_date\", col(\"shipped_date\").cast(\"date\")) \n",
    "    \n",
    "od.write.format(\"org.apache.spark.sql.cassandra\")\\\n",
    "  .mode(\"Append\")\\\n",
    "  .option(\"table\", \"fudgemart_order_details\")\\\n",
    "  .option(\"keyspace\",\"gdemo\")\\\n",
    "  .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c1ec9315-720c-4ae3-9353-0b531a4b4b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df =spark.read.format(\"org.apache.spark.sql.cassandra\")\\\n",
    "    .options(table=\"fudgemart_order_details\", keyspace=\"gdemo\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c9d8c150-9254-4e8e-82ca-0f3efcb51209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: integer (nullable = false)\n",
      " |-- order_id: integer (nullable = false)\n",
      " |-- order_item_id: integer (nullable = true)\n",
      " |-- creditcard_exp_date: string (nullable = true)\n",
      " |-- creditcard_number: string (nullable = true)\n",
      " |-- customer_address: string (nullable = true)\n",
      " |-- customer_city: string (nullable = true)\n",
      " |-- customer_email: string (nullable = true)\n",
      " |-- customer_name: string (nullable = true)\n",
      " |-- customer_state: string (nullable = true)\n",
      " |-- customer_zip: string (nullable = true)\n",
      " |-- order_date: date (nullable = true)\n",
      " |-- order_qty: integer (nullable = true)\n",
      " |-- order_total: decimal(38,18) (nullable = true)\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- product_name: string (nullable = true)\n",
      " |-- product_retail_price: decimal(38,18) (nullable = true)\n",
      " |-- ship_via: string (nullable = true)\n",
      " |-- shipped_date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ebc3a152-0127-45b8-9836-46ceefdc3b01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Project [customer_id#694, order_id#695, order_item_id#696, creditcard_exp_date#697, creditcard_number#698, customer_address#699, customer_city#700, customer_email#701, customer_name#702, customer_state#703, customer_zip#704, order_date#705, order_qty#706, order_total#707, product_id#708, product_name#709, product_retail_price#710, ship_via#711, shipped_date#712]\n",
      "+- BatchScan[customer_id#694, order_id#695, order_item_id#696, creditcard_exp_date#697, creditcard_number#698, customer_address#699, customer_city#700, customer_email#701, customer_name#702, customer_state#703, customer_zip#704, order_date#705, order_qty#706, order_total#707, product_id#708, product_name#709, product_retail_price#710, ship_via#711, shipped_date#712] Cassandra Scan: gdemo.fudgemart_order_details\n",
      " - Cassandra Filters: [[\"customer_id\" = ?, 13],[\"order_id\" = ?, 1843]]\n",
      " - Requested Columns: [customer_id,order_id,order_item_id,creditcard_exp_date,creditcard_number,customer_address,customer_city,customer_email,customer_name,customer_state,customer_zip,order_date,order_qty,order_total,product_id,product_name,product_retail_price,ship_via,shipped_date]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This CAN  be filtered in Cassandra, uses partition key\n",
    "df.filter(\"customer_id=13 and order_id=1843\").explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "931980d6-ffa5-448b-971e-626d6f58c143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Filter (ship_via#711 = Postal Service)\n",
      "+- BatchScan[customer_id#694, order_id#695, order_item_id#696, creditcard_exp_date#697, creditcard_number#698, customer_address#699, customer_city#700, customer_email#701, customer_name#702, customer_state#703, customer_zip#704, order_date#705, order_qty#706, order_total#707, product_id#708, product_name#709, product_retail_price#710, ship_via#711, shipped_date#712] Cassandra Scan: gdemo.fudgemart_order_details\n",
      " - Cassandra Filters: []\n",
      " - Requested Columns: [customer_id,order_id,order_item_id,creditcard_exp_date,creditcard_number,customer_address,customer_city,customer_email,customer_name,customer_state,customer_zip,order_date,order_qty,order_total,product_id,product_name,product_retail_price,ship_via,shipped_date]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This CANNOT be filtered in Cassandra, Optimizations left to Spark. ANTI-PATTERN you can do this, but dont!\n",
    "# Every row of data in your table within this cassandra cluster ends up in your spark cluster!\n",
    "df.filter(\"ship_via='Postal Service'\").explain()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378264ec-a539-4cbe-a8e5-298fed773e98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
