{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba0f4745-b9c7-40a4-a579-fd93b34a9df6",
   "metadata": {},
   "source": [
    "# Unit J\n",
    "# Search Database Model\n",
    "\n",
    "- Examples From Video Lecture \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedd29f1-8425-46f6-b7f0-67e19ef43340",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "# ELASTICSEARCH CONFIGURATION\n",
    "elastic_host = \"elasticsearch\"\n",
    "elastic_port = \"9200\"\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName('jupyter-pyspark') \\\n",
    "    .config(\"spark.jars.packages\",\"org.elasticsearch:elasticsearch-spark-20_2.12:7.15.0\")\\\n",
    "    .config(\"spark.es.nodes\", elastic_host) \\\n",
    "    .config(\"spark.es.port\",elastic_port) \\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a8dd18-fbb4-4dbb-9a46-537517ed6e9f",
   "metadata": {},
   "source": [
    "## Elasticsearch and Kibana\n",
    "\n",
    "### Elasticsearch REST API\n",
    "\n",
    "- Use the HTTP protocol to add and query data in elasticsearch\n",
    "- open a terminal in Jupyter to run these from the Linux command prompt\n",
    "\n",
    "```\n",
    "# add three documents\n",
    "\n",
    "curl -X POST \"http://elasticsearch:9200/people/students\" -H 'Content-Type: application/json' -d '{ \"name\" : \"mike\", \"major\" : \"math\", \"gpa\" : 3.4 }'\n",
    "curl -X POST \"http://elasticsearch:9200/people/students\" -H 'Content-Type: application/json' -d '{ \"name\" : \"phil\", \"major\" : \"math\", \"gpa\" : 3.2 }'\n",
    "curl -X POST \"http://elasticsearch:9200/people/students\" -H 'Content-Type: application/json' -d '{ \"name\" : \"pete\", \"major\" : \"bio\", \"gpa\" : 3.7 }'\n",
    "\n",
    "# Get a count of students (documents)\n",
    "\n",
    "curl -X GET \"http://elasticsearch:9200/_cat/count/people\"\n",
    "\n",
    "#  Find the math majors\n",
    "\n",
    "curl -X GET  \"http://elasticsearch:9200/people/_search?pretty&q=major:math\"\n",
    "```\n",
    "\n",
    "- Type this into your web browser to find students with a GPA>3.3 \n",
    "- NOTE: It's localhost now because we are not inside a docker container.\n",
    "\n",
    "```\n",
    "http://localhost:9200/people/_search?pretty&q=gpa:[3.3 TO *]\n",
    "```\n",
    "\n",
    "### Kibana as a client of Elasticsearch\n",
    " \n",
    "- Kibana serves as an Elasticsearch client.\n",
    "- To view the people/students index, go to Management=> Stack Management=> Index Management\n",
    "    - Summary shows how many documents in the index\n",
    "    - Mappings shows the searchable fields discovered and their data types.\n",
    "- To use in Kibana, we must create a Kibana => Index Pattern\n",
    "    - There are text fiels like major and keyword fields like major.keyword for aggregations\n",
    "    - You can add custom fields by editing the value for example `deanslist` field as `emit( doc['gpa'].value > 3.3)`\n",
    "- Once your Index Pattern is created you can search : Analyticcs => Discover\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61233142-ec3b-47d0-960e-df4ebae44eb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f4c6a39a-d6a8-4576-b918-ae31dba626d7",
   "metadata": {},
   "source": [
    "## Tweet Simulator \n",
    "\n",
    "- Genenrate some fake tweets in quazi-real time as a streaming data source\n",
    "- Each tweet is posted to elasticsearch and echoed to the console\n",
    "- This code will run until the user Stops the Notebook Cell or the Tweet limit is hit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c7f4e6-b63f-43a0-9b09-a1e0481c2d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from simtweet import generateRandomTweet\n",
    "import requests\n",
    "from time import sleep\n",
    "import random\n",
    "import json\n",
    "index = \"tweets\"\n",
    "url = f\"http://{elastic_host}:{elastic_port}/{index}\"\n",
    "headers = { \"Content-Type\" : \"application/json\" }\n",
    "tweet_limit = 100\n",
    "min_delay = 1\n",
    "max_delay = 5\n",
    "\n",
    "for i in range(tweet_limit):\n",
    "    sleep(random.randint(min_delay, max_delay))\n",
    "    tweet = generateRandomTweet()\n",
    "    endpoint = f\"{url}/_doc/{tweet['id']}\"\n",
    "    response = requests.post(endpoint, headers = headers, data = json.dumps(tweet))\n",
    "    response.raise_for_status()\n",
    "    print(f\"curl -X POST {endpoint} -H 'Content-Type: application/json' \\n\\t-d '{json.dumps(tweet)}'\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4998879-4989-4f16-ad87-e0ef1e1345a6",
   "metadata": {},
   "source": [
    "## Spark Elasticsearch\n",
    "\n",
    "The elasticsearch spark connector supports:\n",
    "\n",
    " - Writing Spark  DataFrames to an ES index\n",
    " - An entire ES index into a spark DataFrame\n",
    " - It does NOT support ES Queries.\n",
    " - It does not handle nested schemas without creating a customer mapping in elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082f9f76-0fb6-4d27-b8c5-2f0963b11bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fm = spark.read.option(\"inferSchema\",True).option(\"header\",True).csv(\"file:///home/jovyan/datasets/fudgemart/fudgemart-order-details.csv\")\n",
    "fm.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b709e32e-3945-4228-8279-210ff9ed84b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fm.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab4279d-533b-4c36-8363-6e35e3d1fe46",
   "metadata": {},
   "outputs": [],
   "source": [
    "fm.write.mode(\"Overwrite\").format(\"es\").save(\"fm-order-details/_doc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c848a368-d262-45a1-bc22-3c0a20d38c75",
   "metadata": {},
   "source": [
    "### NOTE: Wait a minute for Elasticsearch to catch up!!!!\n",
    "\n",
    "- Don't read until the mapping is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc02003-e4ef-4934-b44a-d71cc8aa2599",
   "metadata": {},
   "outputs": [],
   "source": [
    "fm2 = spark.read.format(\"es\").load(\"fm-order-details/_doc\")\n",
    "fm2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a8f6bb-eb40-4520-930c-152c277416df",
   "metadata": {},
   "outputs": [],
   "source": [
    " query = fm2.select(\"product_name\",\"product_retail_price\",\"order_qty\").where(\"product_name='Steam Iron'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5644aa-6dd7-4dc2-84ed-c4d7cc540097",
   "metadata": {},
   "outputs": [],
   "source": [
    "query.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e4d266-63b8-4bf6-b74f-08308bfb3be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filters are pushed down into elastic!!\n",
    "fm2 = spark.read.format(\"es\").load(\"fm-order-details/_doc\")\n",
    "fm2.where(\"customer_city='Fresno' and order_total<500\").explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2e688c-a820-4a11-9f22-6d405ef13218",
   "metadata": {},
   "outputs": [],
   "source": [
    "fm2.where(\"customer_city='Fresno' and order_total<500\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0871544a-ced8-4f2d-9ad5-b1b66ff3702c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
